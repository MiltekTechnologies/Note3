= #Summary of work done#
Firos vp 
2020-04 -firosvp333@gmail.com
:toc: left

== *Use git and GitHUb*
==================
>Initialize the local directory as a Git repository.

$ git init

$ git log    

$ git status

>Adds the files in the local repository

$ git add .

>Commits the tracked changes and prepares them to be pushed to a remote repository

$ git commit -m "First commit"

>In the Command prompt, add the URL for the remote repository where your local repository will be pushed.

$ git remote add origin remote repository URL

# Sets the new remote

$ git remote -v

# Verifies the new remote URL

>Push the changes in your local repository to GitHub.

$ git push origin master

$ git push --force

$git push<remote><branch>--force-with-lease

*clone a git repo*

$ git clone URL

*Branching in git*

$ git branch branch name

$ git checkout branch name

to check master branch 

$ git checkout master


image::images/download.png[Git branching,align="center"]

>ultimate Gitignore django

https://djangowaves.com/tips-tricks/gitignore-for-a-django-project/


==================

[dedication]
== *Using ASCIIdoc in vs code*
==================
Note these asciidoc instructions:

    :toc:

    :icons: font

    image::images/download.png[Git branching,400,450,align="center"]

    https://www.w3schools.com/python/python_regex.asp[Regex^]

    [source,python]
    -----------------
    [options="header,footer"]
    |=======================
    |HTTP method    |CRUD       |Action
    |GET            |read       |returns requested data
    |POST           |create     |creates a new record
    |PUT or PATCH   |update     |updates an existing record
    |DELETE         |delete     |deletes an existing record
    |=======================
    

read this for more info

https://marketplace.visualstudio.com/items?itemName=joaompinto.asciidoctor-vscode[AsciiDoc^]

Note :

>Open the command palette - ctrl+shift+p or F1 (Mac: cmd+shift+p).Select AsciiDoc: Save HTML/pdf document.The file is generated in the same folder as the source document

>Toggle Preview - ctrl+shift+v (Mac: cmd+shift+v)

>Open Preview to the Side - ctrl+k v (Mac: cmd+k v)

==================

[dedication]
== *Postgres in grafana*
==================

>In the side menu under the Dashboards link you should find a link named Data Sources.

>Click the + Add data source button in the top header.

>select postgres Sql


image::images/grafanapng.png[Grafana with postgres,align="center"]

>by default some fields of our table comes there..if it doesn't come type manually by using keyboard

image::images/grafana2.png[Grafana with postgres initialization,align="center"]

read 

https://grafana.com/docs/grafana/latest/features/datasources/postgres/

==================

[dedication]
== *Django rest frame work*
==================
important must read 

>request and response >https://www.django-rest-framework.org/tutorial/2-requests-and-responses/

>why serialization? >https://www.django-rest-framework.org/tutorial/1-serialization/

>token authentication >https://simpleisbetterthancomplex.com/tutorial/2018/11/22/how-to-implement-token-authentication-using-django-rest-framework.html

Create a Jason response in django  1.7+

from django.http import JsonResponse
return JsonResponse({'foo':'bar'})

>jason response 

    from django.http import JsonResponse

    def send_json(request):

        data = [{'name': 'Peter', 'email': 'peter@example.org'},
            {'name': 'Julia', 'email': 'julia@example.org'}]

    return JsonResponse(data, safe=False)


=== *AGGREGATION*

>MODEL.objects.filter(Field='value').count()  #to count

>MODEL.objects.aggregate(Avg('field_name'))  #to find avg ....similarly have sum , max and  ..etc

read more

>https://docs.djangoproject.com/en/3.0/topics/db/aggregation/

=== *REGEX*

cool while working with strings...always remember

>https://www.w3schools.com/python/python_regex.asp[Regex^]

=== *MOdels in django*

>save pk 

image::images/pk.png[store pk,align="center"]


>meta option

    class Ox(models.Model):
    horn_length = models.IntegerField()

    class Meta:
        ordering = ["horn_length"]
        verbose_name_plural = "oxen"

Model metadata is “anything that’s not a field”, such as ordering options (ordering), database table name (db_table), or human-readable singular and plural names (verbose_name and verbose_name_plural). None are required, and adding class Meta to a model is completely optiona

image::images/meme2.jpg[fun,400,400,align="center"]


>pk

id = models.AutoField(primary_key=True)

read more

https://docs.djangoproject.com/en/3.0/topics/db/models/

==================

[dedication]
== *Errors happend while running DRF and solution*
==================

>order by descending (NOTE NEGATIVE MARK)

Model.objects.filter(field=field_id).order_by('-field to order')

>delete duplicates rows

[source,python]
-----------------

    for row in MyModel.objects.all().reverse():
        if MyModel.objects.filter(photo_id=row.photo_id).count() > 1:
            row.delete()
-----------------

>Vs code pylint error : change python env and intrepreter


image::images/vscode.png[pylint error,align="center"]

image::images/vscode2.png[pylint error,align="center"]

https://stackoverflow.com/questions/48270385/vs-code-error-when-importing-django-module 


*POSTMAN*


>{"detail": "Authentication credentials were not provided."}

https://stackoverflow.com/questions/26906630/django-rest-framework-authentication-credentials-were-not-provided

>token authentication : https://simpleisbetterthancomplex.com/tutorial/2018/11/22/how-to-implement-token-authentication-using-django-rest-framework.html[this is awsome^]


>always careful about these in general settings of POSTMAN


image::images/postman.png[postman setup,align="center"]

>POST and GET in django api view

https://stackoverflow.com/questions/48901823/post-request-handling-in-django-rest-framework


>could'nt import rest framework django

always use this in settings.py
[source,python]
-----------------

    INSTALLED_APPS = [
   ...
   'rest_framework',
   ....
    ]

    REST_FRAMEWORK = {
    'DEFAULT_AUTHENTICATION_CLASSES': (
        'rest_framework.authentication.SessionAuthentication',
    ),
    }
-----------------
>django-admin.py startproject opens notepad, instead of creating a project

use 'django-admin startproject'

>install psycopg2 in linux

first install $sudo apt install libpq-dev python3-dev

then use pip install psycopg2

>creating a new virtualenv and activate

$python3 -m venv envname

windows:envname-env\Scripts\activate.bat

linux:source tutorial-env/bin/activate


image::images/meme.png[just for fun,400,400,align="center"]

==================

[dedication]
== *Deploy django project with Heroku*
==================

video::kBwhtEIXGII&t=924s[youtube]

https://www.youtube.com/watch?v=kBwhtEIXGII&t=924s


==================

[dedication]
== *REST AND RESTFUL API*
==================
An API is an application programming interface. It is a set of rules that allow programs to talk to each other. The developer creates the API on the server and allows the client to talk to it.

REST determines how the API looks like. It stands for “Representational State Transfer”. It is a set of rules that developers follow when they create their API. One of these rules states that you should be able to get a piece of data (called a resource) when you link to a specific URL.


>Creating a RESTful Web Service

*An Endpoint URL. An application implementing a RESTful API will define one or more URL endpoints with a domain, port, path, and/or querystring — for example, https://mydomain/user/123?format=json.

*The HTTP method. Differing HTTP methods can be used on any endpoint which map to application create, read, update, and delete (CRUD) operations:

[options="header,footer"]
|=======================
|HTTP method    |CRUD       |Action
|GET            |read       |returns requested data
|POST           |create     |creates a new record
|PUT or PATCH   |update     |updates an existing record
|DELETE         |delete     |deletes an existing record
|=======================

*HTTP headers. Information such as authentication tokens or cookies can be contained in the HTTP request header.

*Body Data. Data is normally transmitted in the HTTP body in an identical way to HTML <form> submissions or by sending a single JSON-encoded data string.

>The Response
The response payload can be whatever is practical: data, HTML, an image, an audio file, and so on. Data responses are typically JSON-encoded, but XML, CSV, simple strings, or any other format can be used. You could allow the return format to be specified in the request — for example, /user/123?format=json or /user/123?format=xml.

An appropriate HTTP status code should also be set in the response header. 200 OK is most often used for successful requests, although 201 Created may also be returned when a record is created. Errors should return an appropriate code such as 400 Bad Request, 404 Not Found, 401 Unauthorized, and so on.

==================

[dedication]
== *POSTGIS and GEO-DJANGO FOR LOCATION*
// ==================

=== *Installing POSTGIS* 

Step 1 :Once PostgreSQL is installed, launch Application Stack Builder from (Start->Programs->PostgreSQL 9.5->Applciation Stackbuilder
image::images/postgis1.png[just for fun,600,600,align="center"]

Step 2 : The create spatial database checkbox is optional, and we generally uncheck it. It creates a spatial database for you to experiment with and has all the extensions packaged with PostGIS Bundle

Step 3 : pressyes , yes and yes

Step 4 : CREATE EXTENSION postgis; in sql querry of created database. it will connect postgres with the postgis.

Note:
https://www.bostongis.com/PrinterFriendly.aspx?content_name=postgis_tut01

video::tTUM9XfDvqk[youtube]
https://www.youtube.com/watch?v=tTUM9XfDvqk


=== *Geo-django configuration* 
>Install libraries in windows (Gdal,proj etc.)
> in linux use this commands
==================
$sudo aptitude install gdal-bin libgdal-dev
==================
==================
$ sudo aptitude install python3-gdal
==================
==================
$sudo aptitude install binutils libproj-dev
==================

>in windows download "OSgeo4W". it depends upon python version.

To check python bit version in shell
==================
>>import struct

>>print(struct.calcsize("P") * 8)
==================

>download "OSgeo4W" from  https://trac.osgeo.org/osgeo4w/#:~:text=OSGeo4W%20is%20a%20binary%20distribution,other%20packages%20(over%20150)[Here]

>add settings.py of django like this

[source,python]
-----------------

import os
if os.name == 'nt':
    import platform
    OSGEO4W = r"E:\anc3\envs\ionic\Lib\site-packages\OSGeo4W"
    if '64' in platform.architecture()[0]:
        OSGEO4W += "64"
    assert os.path.isdir(OSGEO4W), "Directory does not exist: " + OSGEO4W
    os.environ['OSGEO4W_ROOT'] = OSGEO4W
    os.environ['GDAL_DATA'] = OSGEO4W + r"\share\gdal"
    os.environ['PROJ_LIB'] = OSGEO4W + r"\share\proj"
    os.environ['PATH'] = OSGEO4W + r"\bin;" + os.environ['PATH']
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
GDAL_LIBRARY_PATH=r'E:\anc3\envs\ionic\Lib\site-packages\OSGeo4W64\bin\gdal300'
-----------------
[source,python]
-----------------
INSTALLED_APPS = [
    # [...]
    'django.contrib.gis'
]
-----------------

image::images/postgis2.png[just for fun,600,600,align="center"]

Refer https://realpython.com/location-based-app-with-geodjango-tutorial/[Important]

[dedication]
== *Slack For Communication*

It’s chat room for your whole company. If you’ve heard of Internet Relay Chat (IRC) before, it’s fairly similar in function. Your team’s Slack will be divided up into smaller ‘channels’ for group discussion, made up of teams, interests or whatever you desire.

@channel

A channel in Slack is like a “room” for discussions, usually arranged around a topic of discussion or the relevant team. To send a notification to everyone in the channel, type @channel and then your message.

=== *Webhook For Slack* 

Step 1:
Create a new Slack app in the workspace where you want to post messages.

Step 2:
From the Features page, toggle Activate Incoming Webhooks on.

Step3:
Click Add New Webhook to Workspace.

Step4:
Pick a channel for the app to post to, then click Authorise.

Step5: Use your Incoming Webhook URL to post a message to Slack. 

Note :

To use webhook for gitlab. Goto project/repository settings> integrations> add slack notification > copy paste the webhook URL and select the channels(only visible on GitLab.com).

image::images/slack1.png[just for fun,600,600,align="center"]

>free slack https://zapier.com/blog/best-free-slack-apps/[apps] 



== *Digital Ocean*


[dedication]
== *OpenCV For Object Detection*

When it comes to deep learning-based object detection there are three primary object detection methods:

1.Faster R-CNN
2.You Only Look Once (YOLO) 
3.Single Shot Detectors (SSDs)
4.MobileNet
5.Cascade Classifier




we will discuss all of them in detail later in the sections.

=== *Haar Cascade Classifier* 

>Haar feature-based cascade classifiers

https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html[go here]

image::images/stages.png[just for fun,600,600,align="center"]

==== Haar Parameters
-------------
cv2.CascadeClassifier.detectMultiScale(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]]) 
-------------

1.scaleFactor : Parameter specifying how much the image size is reduced at each image scale.

2.minNeighbors : Parameter specifying how many neighbors each candidate rectangle should have to retain it. This parameter will affect the quality of the detected faces: higher value results in less detections but with higher quality.

3.flags : Parameter with the same meaning for an old cascade as in the function cvHaarDetectObjects

4.minSize : Minimum possible object size. Objects smaller than that are ignored.

5.maxSize : Maximum possible object size. Objects larger than that are ignored.



eg:-
-----------
faces = face_cascade.detectMultiScale(gray, 1.3, 5)
-----------

sample code : 

-------------
import numpy as np
import cv2
from matplotlib import pyplot as plt

face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')

img = cv2.imread('xfiles4.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

faces = face_cascade.detectMultiScale(gray, 1.3, 5)


for (x,y,w,h) in faces:
    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
    roi_gray = gray[y:y+h, x:x+w]
    roi_color = img[y:y+h, x:x+w]
    eyes = eye_cascade.detectMultiScale(roi_gray)
    for (ex,ey,ew,eh) in eyes:
        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)

cv2.imshow('img',img)
cv2.waitKey(0)
cv2.destroyAllWindows()
-------------

Predefined xml files available https://github.com/opencv/opencv/tree/master/data/haarcascades[here]:


==== Haar training parameter

Look https://docs.opencv.org/3.4/dc/d88/tutorial_traincascade.html[Here]

1.Negatives positive images:

For training we need a set of samples. There are two types of samples: negative and positive. Negative samples correspond to non-object images. Positive samples correspond to images with detected objects.

increase the number of positives will increase the generalization of your model, it will look for better general features and will less likely overfit on your training data. Increasing the number of negative images is needed to remove the large amount of false positive detections.

>Include application specific background information in order to get a good performing detector.

>The larger the dimensions the more features can be calculated and the more weak classifiers will be gathered for the boosting process.

>The dimensions specify the smallest object size you will be able to detect.

So basically more trainingsdata is needed!

2.Cascade parameters

>Haar features

>LBP features

>Type of boosted classifiers: DAB - Discrete AdaBoost, RAB - Real AdaBoost, LB - LogitBoost, GAB - Gentle AdaBoost.

>minHitRate : Minimal desired hit rate for each stage of the classifier. Overall hit rate may be estimated as (min_hit_rate ^ number_of_stages)

>maxFalseAlarmRate :Maximal desired false alarm rate for each stage of the classifier. Overall false alarm rate may be estimated as (max_false_alarm_rate ^ number_of_stages)

>mode <BASIC (default) | CORE | ALL>:Selects the type of Haar features set used in training. BASIC use only upright features, while ALL uses the full set of upright and 45 degree rotated feature se

>maxDepth :Maximal depth of a weak tree. A decent choice is 1, that is case of stumps.

An important site for data sets and papers  https://www.robots.ox.ac.uk/~vgg/data/[site1] and http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm[site2].

=== *YOLO Model in OpenCV* 

==== OpenCV dnn module

>Initially only Caffe and Torch models were supported. Over the period support for different frameworks/libraries like TensorFlow is being added.Support for YOLO/DarkNet has been added recently.

The script requires four input arguments.

1.input image
2.YOLO config file
3.pre-trained YOLO weights
4.text file containing class names

REfer more and Download famous Weights and configuration file from https://pjreddie.com/darknet/yolo/[here].

eg:
==================
$ python yolo_opencv.py --image dog.jpg --config yolov3.cfg --weights yolov3.weights --classes yolov3.txt
==================

https://www.arunponnusamy.com/yolo-object-detection-opencv-python.html[Read more]

== *Face Detection*

=== *Haar Cascade-Face Detection*

Predefined Model https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml[Here]

==== Work Result

image::images/haarface1.png[just for fun,600,600,align="center"]

>Full body detection using Haar 

Paper referred: https://funvision.blogspot.com/2017/01/lbp-cascade-for-head-and-people.html[Here]

Result:

image::images/peoplehaar.png[just for fun,600,600,align="center"]

>HOG Full body detection on same image

Result:

image::images/Hogpeople.png[just for fun,600,600,align="center"]

=== *Custum Haar Model*

>Referred https://www.researchgate.net/publication/338680205_Building_Custom_HAAR-Cascade_Classifier_for_face_Detection[this] paper , Simple and powerful explanation.

>Download Application From https://amin-ahmadi.com/cascade-trainer-gui/[Here]

Result:
>Collected 1000 positive images and 1000 Negative images just for Basic Version of the model. The result was good , But not detecting close-up face. Wanted to change some parameters and re-check the training.(*more updation soon)

=== *YoloV3 Cascade-Face Detection*

>Yolo has a person class in thier predefined model 

https://medium.com/analytics-vidhya/object-detection-with-opencv-python-using-yolov3-481f02c6aa35[Here]

YOLOv3 is the latest variant of a popular object detection algorithm YOLO – You Only Look Once. The published model recognizes *80 different objects* in images and videos, but most importantly it is super fast and nearly as accurate as Single Shot MultiBox (SSD)


== *Hand Gesture Recognition*

*Can be devided into 7 main steps*

>Image Acquisition Module

>Hand detection

>Segmentation

>Feature extraction (Feature Vector)

>Gesture recognition or 
Classification

>Training module 

>Testing and RE-training.

*Or be devided into 3 major steps*

>Hand detection

>Segmentation

>Recognition.

=== *Hand Detection*

mainly Concentrated on 3 models

1.Haar model

2.YOLOV3 model

3.Tensorflow-SavedModel protocol
buffer(.Pb)

==== *Haar Cascade For Hand Detection*

>Download predifened model from https://github.com/Balaje/OpenCV/tree/master/haarcascades[Here]

Result:

image::images/handhaaar.png[just for fun,600,600,align="center"]

==== *Yolov3 Hand detection*

Very high efficiency in detecting when compared to haar model. It can detect any hand poses from a noisy image too. But fps is low than expected.

Referred this https://github.com/cansik/yolo-hand-detection[paper]

Result:

image::images/yolohand.png[just for fun,600,600,align="center"]

------------------
def detect_objects(img, net, outputLayers):			
	blob = cv2.dnn.blobFromImage(img, scalefactor=0.00392, size=(320, 320), mean=(0, 0, 0), swapRB=True, crop=False)
	net.setInput(blob)
	outputs = net.forward(outputLayers)
	return blob, outputs
------------------

>Pass the wieghts and config file to the script.

----------------------------
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
----------------------------

>To reduce multiple rectangles from the window we do Non-maximum Suppression 

--------------------------
indexes = cv2.dnn.NMSBoxes(boxes, confs, 0.5, 0.4)
--------------------------

==== *Hand detection TF (ssd)*

Reffered this https://medium.com/@victor.dibia/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce[Paper]

Frozen Model Available https://github.com/victordibia/handtracking[Here]

>We can do hand tracking in 3 lines of code, no installation, no model training, all in the https://victordibia.github.io/handtrack.js/[browser].

>Or google's Mediapipe is https://google.github.io/mediapipe/[there] For Android,Browser,desktop, python and ios.

>Try MEDIAPIPE https://viz.mediapipe.dev/demo/hand_tracking[Quickly] using Browser

>Sample Code
---------------------
import cv2
import time
# Load a model imported from Tensorflow
tensorflowNet = cv2.dnn.readNetFromTensorflow('frozen_inference_graph.pb','graph1.pbtxt')
 
cap = cv2.VideoCapture(0) 
# Input image
start = time.time()
count=0
while(1):
    ret, frame = cap.read() 
    img = cv2.resize(frame, (640, 480)) 
    rows, cols, channels = img.shape
    
    # Use the given image as input, which needs to be blob(s).
    tensorflowNet.setInput(cv2.dnn.blobFromImage(img, size=(300, 300), swapRB=True, crop=False))
    
    # Runs a forward pass to compute the net output
    networkOutput = tensorflowNet.forward()
    
    # Loop on the outputs
    for detection in networkOutput[0,0]:
        print(detection[2])
        score = float(detection[2])
        print(score)
        if score > 0.6:
            
            left = detection[3] * cols
            top = detection[4] * rows
            right = detection[5] * cols
            bottom = detection[6] * rows
    
            #draw a red rectangle around detected objects
            cv2.rectangle(img, (int(left), int(top)), (int(right), int(bottom)), (0, 0, 255), thickness=2)
    
    # Show the image with a rectagle surrounding the detected objects 
    cv2.imshow('img',img) 
    k = cv2.waitKey(1) & 0xff
    if k == 27: 
        break 
    count+=1
end = time.time()
seconds = end - start
print("Time taken : {0} seconds".format(seconds))
fps  =count / seconds
print ("Estimated frames per second : {0}".format(fps))
cv2.waitKey()
cv2.destroyAllWindows()

---------------------

>Convert keras H5 model to pb https://www.tensorflow.org/guide/keras/save_and_serialize[model]

>If we have .pb file of tensorflow then we are good to go(freezed model preferrable).

>To use in openCV we want an extra file from .pb that is 'pbtxt'. this can be generated from Opencv git https://github.com/opencv/opencv/tree/master/samples/dnn[repo].

>https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API[refer]






























